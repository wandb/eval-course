{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code based evaluation\n",
        "\n",
        "Code-based grading, often referred to as â€œunit testing,â€ â€œheuristic-based evaluation,â€ â€œrule-based evaluation,â€ or â€œprogrammatic evaluation,â€ relies on predefined codeâ€”typically using string matching, regular expressions, or other heuristicsâ€”to assess model outputs. This approach is ideal in scenarios where exact matches or specific key phrases define correctness, as itâ€™s both fast and reliable.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "- **Define Unit Objectives**: Break down what you want to evaluate into specific, testable objectives. IMPORTANT: these objectives can be expressed using a programming language.\n",
        "\n",
        "- **Implement Code Checks**: Write code that verifies whether the modelâ€™s output meets each objective.\n",
        "\n",
        "- **Iterate and Refine**: Continuously improve your evaluation criteria and code based on the modelâ€™s performance and edge cases.\n",
        "\n",
        "### Tips:\n",
        "\n",
        "- **Start Here**: Code-based evaluation is a great starting point for evaluating your LLM application. Itâ€™s straightforward, modular, and allows for quick feedback.\n",
        "\n",
        "- **Refine Your Criteria**: This process often reveals limitations in your evaluation criteria, helping you think critically about what constitutes a â€œgoodâ€ response.\n",
        "\n",
        "- **Keep It Simple**: Focus on keeping evaluations simple and modular, which will make them easier to maintain.\n",
        "\n",
        "- **Integrate with CI/CD**: These unit tests can seamlessly fit into your CI/CD pipeline or act as guardrails, ensuring your applicationâ€™s outputs meet basic standards before deployment."
      ],
      "metadata": {
        "id": "qLS7Cr3fiHCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Run the code cells below to setup your colab notebook."
      ],
      "metadata": {
        "id": "u7Bdo03qrzWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq google-generativeai weave"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50AH1LjIoyHL",
        "outputId": "6f7d842e-3f2b-4b7b-c757-14265d9617ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/301.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.0/301.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/586.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wandb/eval-course"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MxX3qylqzL2",
        "outputId": "3fa96489-b424-49d9-e2ab-127eae9c8532"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'eval-course'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 94 (delta 46), reused 68 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (94/94), 1.06 MiB | 14.85 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/eval-course/notebooks/utils/\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import getpass\n",
        "import weave\n",
        "import pandas as pd\n",
        "\n",
        "# utility script\n",
        "from llm_client import LLMClient\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "_KVMAR6ionYb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Please enter your GOOGLE API KEY with Gemini acccess: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpiC5xMbonZe",
        "outputId": "c3ad32aa-6454-4372-b0c2-f10336052596"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your GOOGLE API KEY with Gemini acccess: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize weave for tracing and evaluation\n",
        "weave_client = weave.init(project_name=\"eval-course/eval-course\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "DYILmSZ7oncj",
        "outputId": "ce10a851-b0db-4166-aab9-0ebc4d2a321e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please login to Weights & Biases (https://wandb.ai/) to continue:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as Weights & Biases user: ayut.\n",
            "View Weave data at https://wandb.ai/eval-course/eval-course/weave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Commit Messages from Code Diffs\n",
        "\n",
        "Imagine youâ€™re working on a project with multiple engineers actively contributing to the same codebase. In a high-velocity environment like this, itâ€™s crucial to maintain clear, informative commit messages to document code changes. Proper commit messages help track code evolution, make debugging easier, and support knowledge transfer across team members.\n",
        "\n",
        "In this example, **weâ€™ll explore using LLMs to automatically generate commit messages based on code diffs**. Automating this process can save time and maintain consistency, but *itâ€™s essential that the generated commit messages meet certain standards*.\n",
        "\n",
        "Weâ€™ll start by generating commit messages for a sample code diff. Then, weâ€™ll demonstrate how to use code-based evaluation to assess whether these messages meet our standards, using simple checks to ensure quality and relevance."
      ],
      "metadata": {
        "id": "g3QUkW-DsLyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Commit Generator Application"
      ],
      "metadata": {
        "id": "-aqse0YD1p0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gemini-1.5-flash-002\"\n",
        "MODEL_CLIENT = \"gemini\""
      ],
      "metadata": {
        "id": "bBkfXkihvgn7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CommitMessageGenerator(weave.Model):\n",
        "    model: LLMClient = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
        "    prompt_template: str = \"\"\"Generate a clear and descriptive commit message for the following code changes.\n",
        "    Format the commit message in the conventional commits style:\n",
        "    <type>(<scope>): <description>\n",
        "\n",
        "    [optional body]\n",
        "\n",
        "    Code diff:\n",
        "    {code_diff}\n",
        "\n",
        "    Focus on:\n",
        "    - What changed?\n",
        "    - Why it changed?\n",
        "    - Any breaking changes\n",
        "    \"\"\"\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, diff: str) -> str:\n",
        "        prompt = self.prompt_template.format(code_diff=diff)\n",
        "        response = self.model.predict(user_prompt=prompt)\n",
        "        return response.strip()"
      ],
      "metadata": {
        "id": "X7JsFVLQonek"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff_example_1 = \"\"\"\n",
        "diff --git a/src/auth.py b/src/auth.py\n",
        "index abc123..def456 100644\n",
        "--- a/src/auth.py\n",
        "+++ b/src/auth.py\n",
        "@@ -10,6 +10,12 @@ class AuthManager:\n",
        "     def validate_token(self, token):\n",
        "         return self.jwt.decode(token, self.secret_key)\n",
        "\n",
        "+    def refresh_token(self, old_token):\n",
        "+        if not self.validate_token(old_token):\n",
        "+            raise InvalidTokenError\n",
        "+        user_data = self.jwt.decode(old_token)\n",
        "+        return self.generate_token(user_data)\n",
        "\"\"\"\n",
        "\n",
        "commit_msg_generator = CommitMessageGenerator()\n",
        "commit_msg_1 = commit_msg_generator.predict(diff_example_1)\n",
        "print(commit_msg_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "vQ0hr44Uonhr",
        "outputId": "1b086c36-047f-4ab4-839e-6ab775ec5a88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ© https://wandb.ai/eval-course/eval-course/r/call/0192fcdd-ee97-7d63-a585-e412b379af14\n",
            "```\n",
            "feat(auth): Add token refresh functionality\n",
            "\n",
            "Adds a `refresh_token` method to the `AuthManager` class. This allows clients to obtain a new JWT by providing a valid existing token.  This improves the user experience by extending session lifetimes without requiring repeated logins.  The new method validates the old token; if invalid, it raises an `InvalidTokenError`.\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Code based evaluation\n",
        "\n",
        "In this section, we will define a few objective criterias and write a programmatic (no use of LLMs) functions to evaluate the quality of the commit messages.\n",
        "\n",
        "A good commit message on a high-level should:\n",
        "\n",
        "- Summarize the changes accurately and concisely.\n",
        "\n",
        "- Highlight key functions, methods, or modules affected.\n",
        "\n",
        "- Be free of unnecessary information or â€œfluff.â€\n",
        "\n",
        "Below we are converting these high-level concepts into unit objectives.\n",
        "\n",
        "Are these objectives \"actually\" capturing the full extent of the \"quality\" measure of the generated commit messages? In this case, it is not.\n",
        "\n",
        "But the main selling point is \"the speed of writing few criterias/objectives as function and the speed of running them\"."
      ],
      "metadata": {
        "id": "s4CWeeFjPSks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define objectives as functions\n",
        "\n",
        "# @weave.op()\n",
        "def follows_conventional_format(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message follows conventional commit format\"\"\"\n",
        "    conv_commit_pattern = r'^(feat|fix|perf|refactor|style|test|docs|build|ci|chore)(\\([a-z-]+\\))?: .+'\n",
        "    return bool(re.match(conv_commit_pattern, model_output.split('\\n')[0]))\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def length_appropriate(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message length is appropriate (between 10-72 chars)\"\"\"\n",
        "    first_line = model_output.split('\\n')[0]\n",
        "    return 10 <= len(first_line) <= 72\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def contains_key_components(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message contains key components (what and why)\"\"\"\n",
        "    return (\n",
        "        any(word in model_output.lower() for word in [\"add\", \"update\", \"fix\", \"remove\", \"implement\"]) and\n",
        "        (\"to\" in model_output.lower() or \"for\" in model_output.lower() or \"because\" in model_output.lower())\n",
        "    )\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def no_generic_terms(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message avoids generic terms\"\"\"\n",
        "    generic_terms = [\"stuff\", \"things\", \"updated\", \"fixed\", \"changed\"]\n",
        "    return not any(term in model_output.lower() for term in generic_terms)\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_imperative_mood(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message uses imperative mood (starts with verb)\"\"\"\n",
        "    first_word = model_output.split('\\n')[0].split()[0].lower()\n",
        "    imperative_verbs = [\"add\", \"update\", \"fix\", \"remove\", \"implement\", \"change\", \"refactor\", \"optimize\", \"delete\", \"create\"]\n",
        "    return any(first_word == verb for verb in imperative_verbs)\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_proper_capitalization(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message follows proper capitalization (first letter capitalized, no period)\"\"\"\n",
        "    first_line = model_output.split('\\n')[0]\n",
        "    return (first_line[0].isupper() and\n",
        "            not first_line.endswith('.'))\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_scope_if_needed(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message includes scope when appropriate\"\"\"\n",
        "    first_line = model_output.split('\\n')[0]\n",
        "    type_with_scope = r'^(feat|fix|refactor)\\([a-z-]+\\): '\n",
        "    type_without_scope = r'^(docs|test|style|chore): '\n",
        "    return bool(re.match(type_with_scope, first_line) or re.match(type_without_scope, first_line))\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_detailed_body_if_complex(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message has detailed body for complex changes\"\"\"\n",
        "    lines = model_output.split('\\n')\n",
        "    # Complex changes indicated by certain keywords\n",
        "    complex_indicators = [\"refactor\", \"breaking\", \"deprecate\", \"remove\", \"!:\"]\n",
        "    is_complex = any(indicator in lines[0].lower() for indicator in complex_indicators)\n",
        "\n",
        "    if is_complex:\n",
        "        # Should have at least one line of body text after blank line\n",
        "        return len(lines) >= 3 and lines[1].strip() == \"\" and any(line.strip() for line in lines[2:])\n",
        "    return True"
      ],
      "metadata": {
        "id": "yNWNtzybonjj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have synthetically generated a dataset of code diffs. Let's load it and see what it looks like.\n",
        "\n",
        "In pratice, you can build this  diffs from your existing code"
      ],
      "metadata": {
        "id": "_ADIJxrZQUWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_diffs_dataset = weave.ref('weave:///eval-course/eval-course-dev/object/code-diffs:JJTbwBlIr6YqYARd7Yt3epxHkYLwXf7u5YxiYy2vJ7w').get()\n",
        "print(\"Total number of samples: \", len(code_diffs_dataset.rows))\n",
        "\n",
        "print(code_diffs_dataset.rows[0][\"diff\"], sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ73sF0m1Npg",
        "outputId": "d30837c4-ae8f-4de0-85a9-e7bcfea2c677"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples:  10\n",
            "\n",
            "diff --git a/src/user.py b/src/user.py\n",
            "index abc123..def456 100644\n",
            "--- a/src/user.py\n",
            "+++ b/src/user.py\n",
            "@@ -10,6 +10,9 @@ class User:\n",
            "     def get_name(self):\n",
            "         return self.name\n",
            " \n",
            "+    def get_email(self):\n",
            "+        return self.email\n",
            "+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we are not concerned about \"gold\" standard commit messages here. We have the user query - in the form of code diffs. We will evaluate the quality of the commit messages generated by LLMs directly using the above defined criterias. This is the beauty and one of the pros of code based evaluation.\n",
        "\n",
        "Below I am collecting all the different code based criterias under one `Scorer`. The `summarize` method will run at the end of the scoring process to aggregate the scores. If you don't write this method, `auto_summarize` will be called by default. The example below shows how to structure your code evaluation logic along with custom aggregation logic."
      ],
      "metadata": {
        "id": "m8wZb4tR3BTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from weave import Scorer\n",
        "\n",
        "\n",
        "class CodeDiffScorer(Scorer):\n",
        "    @weave.op()\n",
        "    def score(self, model_output: str) -> dict:\n",
        "        result = {\n",
        "            \"follows_conventional_format\": follows_conventional_format(model_output),\n",
        "            \"length_appropriate\": length_appropriate(model_output),\n",
        "            \"contains_key_components\": contains_key_components(model_output),\n",
        "            \"no_generic_terms\": no_generic_terms(model_output),\n",
        "            \"has_imperative_mood\": has_imperative_mood(model_output),\n",
        "            \"has_proper_capitalization\": has_proper_capitalization(model_output),\n",
        "            \"has_scope_if_needed\": has_scope_if_needed(model_output),\n",
        "            \"has_detailed_body_if_complex\": has_detailed_body_if_complex(model_output),\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    @weave.op()\n",
        "    def summarize(self, score_rows: list) -> Optional[dict]:\n",
        "        if not score_rows:\n",
        "            return None\n",
        "\n",
        "        # Initialize counters for each metric with weight 1\n",
        "        metrics = {\n",
        "            'follows_conventional_format': {'weight': 1, 'count': 0},\n",
        "            'length_appropriate': {'weight': 1, 'count': 0},\n",
        "            'contains_key_components': {'weight': 1, 'count': 0},\n",
        "            'no_generic_terms': {'weight': 1, 'count': 0},\n",
        "            'has_imperative_mood': {'weight': 1, 'count': 0},\n",
        "            'has_proper_capitalization': {'weight': 1, 'count': 0},\n",
        "            'has_scope_if_needed': {'weight': 1, 'count': 0},\n",
        "            'has_detailed_body_if_complex': {'weight': 1, 'count': 0}\n",
        "        }\n",
        "\n",
        "        # Sum up scores for each metric\n",
        "        total = len(score_rows)\n",
        "        for row in score_rows:\n",
        "            for metric in metrics:\n",
        "                if row[metric]:\n",
        "                    metrics[metric]['count'] += 1\n",
        "\n",
        "        # Calculate weighted average score\n",
        "        weighted_sum = sum(\n",
        "            (metrics[metric]['count'] / total) * metrics[metric]['weight']\n",
        "            for metric in metrics\n",
        "        )\n",
        "        total_weights = sum(metrics[metric]['weight'] for metric in metrics)\n",
        "        code_eval_score = weighted_sum / total_weights\n",
        "\n",
        "        summary = {'code_eval_score': code_eval_score}\n",
        "\n",
        "        return summary\n",
        "\n",
        "code_evaluator = CodeDiffScorer()"
      ],
      "metadata": {
        "id": "6IkieePh1Nqi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the evaluation."
      ],
      "metadata": {
        "id": "JuR7oUwh3TGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from weave import Evaluation\n",
        "\n",
        "# Create evaluation\n",
        "evaluation = Evaluation(\n",
        "    dataset=code_diffs_dataset,\n",
        "    scorers=[code_evaluator]\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "asyncio.run(evaluation.evaluate(CommitMessageGenerator()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "CRr_xpod1Nrr",
        "outputId": "2eb73739-07fb-4408-a1c7-8195672c03cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation summary\n",
              "\u001b[1m{\u001b[0m\u001b[32m'CodeDiffScorer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'code_eval_score'\u001b[0m: \u001b[1;36m0.325\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m9.362474131584168\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
              "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'CodeDiffScorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'code_eval_score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.325</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.362474131584168</span><span style=\"font-weight: bold\">}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ© https://wandb.ai/eval-course/eval-course/r/call/0192fcde-1c50-71a0-a970-fbf8d2c9ecc2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CodeDiffScorer': {'code_eval_score': 0.325},\n",
              " 'model_latency': {'mean': 9.362474131584168}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}