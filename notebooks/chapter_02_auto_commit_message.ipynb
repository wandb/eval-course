{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLS7Cr3fiHCg"
      },
      "source": [
        "# Chapter 2: Programmatic and LLM Evaluations\n",
        "\n",
        "This code notebook is part of Chapter 2 lesson of the [LLM Apps: Evaluation course](https://wandb.ai/site/courses/evals/).\n",
        "\n",
        "# How basic evals can improve your LLM system?\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/eval-course/blob/main/notebooks/chapter_02_auto_commit_message.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<!--- @wandbcode{eval-course-02} -->\n",
        "\n",
        "Programmatic evaluation is easy to setup and can help improve your LLM system. It's a low hanging fruit and thus best to start from here.\n",
        "\n",
        "Code-based grading, often referred to as “unit testing,” “heuristic-based evaluation,” “rule-based evaluation,” “programmatic evaluation,” or \"assertion based evaluation,\" relies on predefined code—typically using string matching, regular expressions, or other heuristics—to assess model outputs. This approach is ideal in scenarios where exact matches or specific key phrases define correctness, as it’s both fast and reliable.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "- **Define Unit Objectives**: Break down what you want to evaluate into specific, testable objectives. IMPORTANT: these objectives can be expressed using a programming language.\n",
        "\n",
        "- **Implement Code Checks**: Write code that verifies whether the model’s output meets each objective.\n",
        "\n",
        "- **Iterate and Refine**: Continuously improve your evaluation criteria and code based on the model’s performance and edge cases.\n",
        "\n",
        "### Tips:\n",
        "\n",
        "- **Start Here**: Code-based evaluation is a great starting point for evaluating your LLM application. It’s straightforward, modular, and allows for quick feedback.\n",
        "\n",
        "- **Refine Your Criteria**: This process often reveals limitations in your evaluation criteria, helping you think critically about what constitutes a “good” response.\n",
        "\n",
        "- **Keep It Simple**: Focus on keeping evaluations simple and modular, which will make them easier to maintain.\n",
        "\n",
        "- **Integrate with CI/CD**: These unit tests can seamlessly fit into your CI/CD pipeline or act as guardrails, ensuring your application’s outputs meet basic standards before deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Bdo03qrzWH"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the code cells below to setup your colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone --branch main https://github.com/wandb/eval-course\n",
        "    %cd eval-course\n",
        "    %cd notebooks\n",
        "else:\n",
        "    print(\"Not running in Google Colab. Skipping git clone.\")\n",
        "\n",
        "!pip install -qq google-generativeai weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KVMAR6ionYb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import getpass\n",
        "import weave\n",
        "import pandas as pd\n",
        "\n",
        "# utility script\n",
        "from utils.llm_client import LLMClient\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpiC5xMbonZe"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Please enter your GOOGLE API KEY with Gemini acccess: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYILmSZ7oncj"
      },
      "outputs": [],
      "source": [
        "# initialize weave for tracing and evaluation\n",
        "weave_client = weave.init(project_name=\"eval-course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3QUkW-DsLyl"
      },
      "source": [
        "## Generate Commit Messages from Code Diffs\n",
        "\n",
        "Imagine you’re working on a project with multiple engineers actively contributing to the same codebase. In a high-velocity environment like this, it’s crucial to maintain clear, informative commit messages to document code changes. Proper commit messages help track code evolution, make debugging easier, and support knowledge transfer across team members.\n",
        "\n",
        "In this example, **we’ll explore using LLMs to automatically generate commit messages based on code diffs**. Automating this process can save time and maintain consistency, but *it’s essential that the generated commit messages meet certain standards*.\n",
        "\n",
        "We’ll start by generating commit messages for a sample code diff. Then, we’ll demonstrate how to use code-based evaluation to assess whether these messages meet our standards, using simple checks to ensure quality and relevance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aqse0YD1p0H"
      },
      "source": [
        "### Part 1: Commit Generator Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBkfXkihvgn7"
      },
      "outputs": [],
      "source": [
        "MODEL = \"gemini-2.0-flash-exp\"\n",
        "MODEL_CLIENT = \"gemini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7JsFVLQonek"
      },
      "outputs": [],
      "source": [
        "class CommitMessageGenerator(weave.Model):\n",
        "    model: LLMClient = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
        "    prompt_template: str = \"\"\"Generate a clear and descriptive commit message for the following code changes.\n",
        "    Format the commit message in the conventional commits style:\n",
        "    <type>(<scope>): <description>\n",
        "\n",
        "    [optional body]\n",
        "\n",
        "    Code diff:\n",
        "    {code_diff}\n",
        "\n",
        "    Focus on:\n",
        "    - What changed?\n",
        "    - Why it changed?\n",
        "    - Any breaking changes\n",
        "    \"\"\"\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, diff: str) -> str:\n",
        "        prompt = self.prompt_template.format(code_diff=diff)\n",
        "        response = self.model.predict(user_prompt=prompt)\n",
        "        return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ0hr44Uonhr"
      },
      "outputs": [],
      "source": [
        "diff_example_1 = \"\"\"\n",
        "diff --git a/src/auth.py b/src/auth.py\n",
        "index abc123..def456 100644\n",
        "--- a/src/auth.py\n",
        "+++ b/src/auth.py\n",
        "@@ -10,6 +10,12 @@ class AuthManager:\n",
        "     def validate_token(self, token):\n",
        "         return self.jwt.decode(token, self.secret_key)\n",
        "\n",
        "+    def refresh_token(self, old_token):\n",
        "+        if not self.validate_token(old_token):\n",
        "+            raise InvalidTokenError\n",
        "+        user_data = self.jwt.decode(old_token)\n",
        "+        return self.generate_token(user_data)\n",
        "\"\"\"\n",
        "\n",
        "commit_msg_generator = CommitMessageGenerator()\n",
        "commit_msg_1 = commit_msg_generator.predict(diff_example_1)\n",
        "print(commit_msg_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4CWeeFjPSks"
      },
      "source": [
        "### Part 2: Code based evaluation\n",
        "\n",
        "In this section, we will define a few objective criterias and write a programmatic (no use of LLMs) functions to evaluate the quality of the commit messages.\n",
        "\n",
        "A good commit message on a high-level should:\n",
        "\n",
        "- Summarize the changes accurately and concisely.\n",
        "\n",
        "- Highlight key functions, methods, or modules affected.\n",
        "\n",
        "- Be free of unnecessary information or “fluff.”\n",
        "\n",
        "Below we are converting these high-level concepts into unit objectives.\n",
        "\n",
        "Are these objectives \"actually\" capturing the full extent of the \"quality\" measure of the generated commit messages? In this case, it is not.\n",
        "\n",
        "But the main selling point is \"the speed of writing few criterias/objectives as function and the speed of running them\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNWNtzybonjj"
      },
      "outputs": [],
      "source": [
        "# Define objectives as functions\n",
        "\n",
        "# @weave.op()\n",
        "def follows_conventional_format(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message follows conventional commit format\"\"\"\n",
        "    conv_commit_pattern = r'^(feat|fix|perf|refactor|style|test|docs|build|ci|chore)(\\([a-z-]+\\))?: .+'\n",
        "    return bool(re.match(conv_commit_pattern, model_output.split('\\n')[0]))\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def length_appropriate(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message length is appropriate (between 10-72 chars)\"\"\"\n",
        "    first_line = model_output.split('\\n')[0]\n",
        "    return 10 <= len(first_line) <= 72\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def contains_key_components(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message contains key components (what and why)\"\"\"\n",
        "    return (\n",
        "        any(word in model_output.lower() for word in [\"add\", \"update\", \"fix\", \"remove\", \"implement\"]) and\n",
        "        (\"to\" in model_output.lower() or \"for\" in model_output.lower() or \"because\" in model_output.lower())\n",
        "    )\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def no_generic_terms(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message avoids generic terms\"\"\"\n",
        "    generic_terms = [\"stuff\", \"things\"]\n",
        "    return not any(term in model_output.lower() for term in generic_terms)\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_imperative_mood(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message uses imperative mood (starts with verb)\"\"\"\n",
        "    first_word = model_output.split('\\n')[0].split()[0].lower()\n",
        "    imperative_verbs = [\"add\", \"update\", \"fix\", \"remove\", \"implement\", \"change\", \"refactor\", \"optimize\", \"delete\", \"create\"]\n",
        "    return any(first_word == verb for verb in imperative_verbs)\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_proper_capitalization(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message follows proper capitalization (first letter capitalized, no period)\"\"\"\n",
        "    first_line = model_output.split('\\n')[0]\n",
        "    return (first_line[0].isupper() and\n",
        "            not first_line.endswith('.'))\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_scope_if_needed(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message includes scope when appropriate\"\"\"\n",
        "    first_line = model_output.split('\\n')[0]\n",
        "    type_with_scope = r'^(feat|fix|refactor)\\([a-z-]+\\): '\n",
        "    type_without_scope = r'^(docs|test|style|chore): '\n",
        "    return bool(re.match(type_with_scope, first_line) or re.match(type_without_scope, first_line))\n",
        "\n",
        "\n",
        "# @weave.op()\n",
        "def has_detailed_body_if_complex(model_output: str) -> bool:\n",
        "    \"\"\"Check if commit message has detailed body for complex changes\"\"\"\n",
        "    lines = model_output.split('\\n')\n",
        "    # Complex changes indicated by certain keywords\n",
        "    complex_indicators = [\"refactor\", \"breaking\", \"deprecate\", \"remove\", \"!:\"]\n",
        "    is_complex = any(indicator in lines[0].lower() for indicator in complex_indicators)\n",
        "\n",
        "    if is_complex:\n",
        "        # Should have at least one line of body text after blank line\n",
        "        return len(lines) >= 3 and lines[1].strip() == \"\" and any(line.strip() for line in lines[2:])\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ADIJxrZQUWe"
      },
      "source": [
        "I have synthetically generated a dataset of code diffs. Let's load it and see what it looks like.\n",
        "\n",
        "In pratice, you can build this  diffs from your existing code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ73sF0m1Npg"
      },
      "outputs": [],
      "source": [
        "code_diffs_dataset = weave.ref('weave:///eval-course/eval-course-dev/object/code-diffs:JJTbwBlIr6YqYARd7Yt3epxHkYLwXf7u5YxiYy2vJ7w').get()\n",
        "print(\"Total number of samples: \", len(code_diffs_dataset.rows))\n",
        "\n",
        "print(code_diffs_dataset.rows[0][\"diff\"], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8wZb4tR3BTi"
      },
      "source": [
        "Note that we are not concerned about \"gold\" standard commit messages here. We have the user query - in the form of code diffs. We will evaluate the quality of the commit messages generated by LLMs directly using the above defined criterias. This is the beauty and one of the pros of code based evaluation.\n",
        "\n",
        "Below I am collecting all the different code based criterias under one `Scorer`. The `summarize` method will run at the end of the scoring process to aggregate the scores. If you don't write this method, `auto_summarize` will be called by default. The example below shows how to structure your code evaluation logic along with custom aggregation logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8XA3Ya7hzhJ"
      },
      "outputs": [],
      "source": [
        "# Let's test if our criterias are working!\n",
        "def _compute_criteria_wise_scores(output: str):\n",
        "    scores = {\n",
        "        \"follows_conventional_format\": follows_conventional_format(output),\n",
        "        \"length_appropriate\": length_appropriate(output),\n",
        "        \"contains_key_components\": contains_key_components(output),\n",
        "        \"no_generic_terms\": no_generic_terms(output),\n",
        "        \"has_imperative_mood\": has_imperative_mood(output),\n",
        "        \"has_proper_capitalization\": has_proper_capitalization(output),\n",
        "        \"has_scope_if_needed\": has_scope_if_needed(output),\n",
        "        \"has_detailed_body_if_complex\": has_detailed_body_if_complex(output),\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "_compute_criteria_wise_scores(commit_msg_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IkieePh1Nqi"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from weave import Scorer\n",
        "\n",
        "\n",
        "class CodeDiffScorer(Scorer):\n",
        "    @weave.op()\n",
        "    def score(self, output: str) -> dict:\n",
        "        # compute scores row wise\n",
        "        result = _compute_criteria_wise_scores(output)\n",
        "        return result\n",
        "\n",
        "    @weave.op()\n",
        "    def summarize(self, score_rows: list) -> Optional[dict]:\n",
        "        # aggregate the results\n",
        "        if not score_rows:\n",
        "            return None\n",
        "\n",
        "        # Initialize counters for each metric with weight 1\n",
        "        metrics = {\n",
        "            'follows_conventional_format': {'weight': 1, 'count': 0},\n",
        "            'length_appropriate': {'weight': 1, 'count': 0},\n",
        "            'contains_key_components': {'weight': 1, 'count': 0},\n",
        "            'no_generic_terms': {'weight': 1, 'count': 0},\n",
        "            'has_imperative_mood': {'weight': 1, 'count': 0},\n",
        "            'has_proper_capitalization': {'weight': 1, 'count': 0},\n",
        "            'has_scope_if_needed': {'weight': 1, 'count': 0},\n",
        "            'has_detailed_body_if_complex': {'weight': 1, 'count': 0}\n",
        "        }\n",
        "\n",
        "        # Sum up scores for each metric\n",
        "        total = len(score_rows)\n",
        "        for row in score_rows:\n",
        "            for metric in metrics:\n",
        "                if row[metric]:\n",
        "                    metrics[metric]['count'] += 1\n",
        "\n",
        "        # Calculate criteria wise score\n",
        "        criteria_wise_score = {}\n",
        "        for metric_name in metrics:\n",
        "            criteria_wise_score[metric_name] = metrics[metric_name]['count'] / total\n",
        "\n",
        "        # Calculate weighted average score\n",
        "        weighted_sum = sum(\n",
        "            (metrics[metric]['count'] / total) * metrics[metric]['weight']\n",
        "            for metric in metrics\n",
        "        )\n",
        "        total_weights = sum(metrics[metric]['weight'] for metric in metrics)\n",
        "        code_eval_score = weighted_sum / total_weights\n",
        "\n",
        "        summary = {'code_eval_score': code_eval_score}\n",
        "        summary.update(criteria_wise_score)\n",
        "\n",
        "        print(summary)\n",
        "\n",
        "        return summary\n",
        "\n",
        "code_evaluator = CodeDiffScorer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuR7oUwh3TGG"
      },
      "source": [
        "Let's run the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRr_xpod1Nrr"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from weave import Evaluation\n",
        "\n",
        "# Create evaluation\n",
        "evaluation = Evaluation(\n",
        "    dataset=code_diffs_dataset.rows,\n",
        "    scorers=[code_evaluator]\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "asyncio.run(evaluation.evaluate(CommitMessageGenerator()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkbM4jQOoAaW"
      },
      "source": [
        "### Part 3: Improve the application with learnings\n",
        "\n",
        "Doing the exercise of writing code based evaluation criterias can in turn help improve the application. Below, I have added instructions in the `prompt_template` that in theory should help improve the `code_eval_score` metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMewTfU2n0QY"
      },
      "outputs": [],
      "source": [
        "class CommitMessageGenerator(weave.Model):\n",
        "    model: LLMClient = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
        "    prompt_template: str = \"\"\"Generate a clear and descriptive commit message for the following code changes.\n",
        "    Format the commit message in the conventional commits style:\n",
        "    <type>(<scope>): <description>\n",
        "\n",
        "    [optional body]\n",
        "\n",
        "    Code diff:\n",
        "    {code_diff}\n",
        "\n",
        "    Focus on:\n",
        "    - What changed?\n",
        "    - Why it changed?\n",
        "    - Any breaking changes\n",
        "\n",
        "    Make sure to follow these instructions:\n",
        "\n",
        "    - Format Compliance: Begin with a type (e.g., feat, fix), optional scope, and a colon.\n",
        "    - Length: Keep the main message between 10-72 characters.\n",
        "    - Key Components: Include what changed and why (e.g., “add… to improve…”).\n",
        "    - Avoid Generic Terms: Avoid vague words like “stuff,” “updated,” or “changed.”\n",
        "    - Imperative Mood: Start with an action verb in imperative form (e.g., “add,” “fix”).\n",
        "    - Capitalization: Capitalize the first word; avoid ending with a period.\n",
        "    - Scope Usage: Use a scope if relevant (e.g., feat(auth): ...).\n",
        "    - Detailed Body: For complex changes, include a detailed body after a blank line.\n",
        "    \"\"\"\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, diff: str) -> str:\n",
        "        prompt = self.prompt_template.format(code_diff=diff)\n",
        "        response = self.model.predict(user_prompt=prompt)\n",
        "        return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rRx31CVpCb8"
      },
      "outputs": [],
      "source": [
        "asyncio.run(evaluation.evaluate(CommitMessageGenerator()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWz-YZucqpAJ"
      },
      "source": [
        "In practice, this is an iterative cycyle. This colab notebook is showing just the first iteration but I hope you get the point. :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
