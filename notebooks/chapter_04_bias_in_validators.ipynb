{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFBWvJ1GzKd5"
      },
      "source": [
        "# Chapter 4: Improving LLM Evaluators\n",
        "This code notebook is part of Chapter 4 lesson of the [LLM Apps: Evaluation course](https://wandb.ai/site/courses/evals/).\n",
        "\n",
        "## Bias in LLM Evaluators\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/eval-course/blob/main/notebooks/chapter_04_bias_in_validators.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<!--- @wandbcode{eval-course-04} -->\n",
        "\n",
        "LLM evaluators are incredibly effective tools for automating evaluation tasks, but they are not without their limitations. Like all LLM-based applications, they are susceptible to biases—both subtle and explicit. These biases don’t stem inherently from the concept of LLM evaluators themselves, but rather reflect the underlying patterns in the data and training processes that power modern LLMs.\n",
        "\n",
        "Understanding and addressing these biases is crucial because they can distort evaluation outcomes, undermine fairness, or misalign with human judgment. While these issues are artifacts of today’s LLM systems—products of imperfect datasets, model training dynamics, and real-world complexities—they represent challenges we must navigate thoughtfully. Importantly, ongoing advancements in model development and data curation could significantly reduce or eliminate these biases in the future.\n",
        "\n",
        "In this section, we’ll unpack common types of biases in LLM evaluators, demonstrate their real-world impact, and explore best practices to mitigate these biases, ensuring that evaluations remain reliable and aligned with desired objectives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU5_ys-I0Aik"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the code cells below to setup your colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone --branch main https://github.com/wandb/eval-course\n",
        "    %cd eval-course\n",
        "    %cd notebooks\n",
        "else:\n",
        "    print(\"Not running in Google Colab. Skipping git clone.\")\n",
        "\n",
        "!pip install -qq google-generativeai weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ddn8rt254IF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import getpass\n",
        "import weave\n",
        "import pandas as pd\n",
        "\n",
        "# utility script\n",
        "from utils.llm_client import LLMClient\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jXshAEO55gZ"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(\n",
        "    api_key=getpass.getpass(\"Please enter your GOOGLE API KEY with Gemini acccess: \"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DpDdDvb57Ik"
      },
      "outputs": [],
      "source": [
        "# initialize weave for tracing and evaluation\n",
        "weave_client = weave.init(project_name=\"eval-course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAoJ2J7W8e9g"
      },
      "source": [
        "## Problem 1: Position Bias\n",
        "\n",
        "LLM validators might favor outputs based on their position (early or late in a sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5Ki-fBZ8H76"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "from weave import Evaluation, Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIy1U9tN9nPB"
      },
      "outputs": [],
      "source": [
        "# Define the prompt template for pairwise comparison\n",
        "PAIRWISE_PROMPT = \"\"\"Given a math question and two possible answers, determine which answer is better.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer A: {answer_a}\n",
        "Answer B: {answer_b}\n",
        "\n",
        "Which answer is better? Respond with JUST \"A\" or \"B\".\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class PairWiseEvaluator(Model):\n",
        "    where_is_correct: str = \"A\"\n",
        "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "    pairwise_judge_prompt: str = PAIRWISE_PROMPT\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str, correct: str, incorrect: str) -> dict:\n",
        "        if self.where_is_correct == \"A\":\n",
        "            response = self.model.generate_content(\n",
        "                self.pairwise_judge_prompt.format(\n",
        "                    question=question, answer_a=correct, answer_b=incorrect,\n",
        "                ),\n",
        "            )\n",
        "        elif self.where_is_correct == \"B\":\n",
        "            response = self.model.generate_content(\n",
        "                self.pairwise_judge_prompt.format(\n",
        "                    question=question, answer_a=incorrect, answer_b=correct,\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"where_is_correct must be either 'A' or 'B'\")\n",
        "\n",
        "        result = response.text.strip(\" \\n\")\n",
        "        return self.where_is_correct, result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9Ozdmdy9n2q"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "mmlu_maths = weave.ref(\n",
        "    \"weave:///eval-course/eval-course-dev/object/mmlu_maths:sJp05YkihutzRAf3YZVXrvLUrN1qj49GvCKTgOoVSlE\"\n",
        ").get()\n",
        "\n",
        "# Metric\n",
        "@weave.op()\n",
        "def exact_match(output: tuple) -> bool:\n",
        "    \"\"\"Check if predicted score matches human score\"\"\"\n",
        "    where_is_correct, result = output\n",
        "    return where_is_correct == result\n",
        "\n",
        "# Create evaluation\n",
        "evaluation = Evaluation(dataset=mmlu_maths.rows, scorers=[exact_match])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TEPe6yG9n0W"
      },
      "outputs": [],
      "source": [
        "# Run evaluation with where_is_correct = \"A\"\n",
        "pairwise_evaluator = PairWiseEvaluator(where_is_correct=\"A\")\n",
        "a = asyncio.run(evaluation.evaluate(pairwise_evaluator))\n",
        "\n",
        "# Run evaluation with where_is_correct = \"B\"\n",
        "pairwise_evaluator = PairWiseEvaluator(where_is_correct=\"B\")\n",
        "b = asyncio.run(evaluation.evaluate(pairwise_evaluator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVX5lI4SFHC3"
      },
      "source": [
        "What's the difference between the two evaluations?\n",
        "\n",
        "For the same question, the evaluator is more likely to choose the answer based on the position of the answer in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8VzEIfq9ny8"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"What's the difference in acccuracy becasue of position bias?\\n\",\n",
        "    b[\"exact_match\"][\"true_fraction\"] - a[\"exact_match\"][\"true_fraction\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650ovT_yJdLb"
      },
      "source": [
        "### Solutions\n",
        "\n",
        "- Swap Augmentation: Randomize the order of outputs to minimize position bias.\n",
        "    - This is espically useful if you run your evaluation multiple times and take the average. ([Source](https://arxiv.org/pdf/2306.05685))\n",
        "\n",
        "- Multiple Evidence Calibration (MEC): Prompt the model to generate evaluation evidence before assigning scores. In simple terms, you are asking the model to reason about the quality of the answer before assigning a score. ([Source](https://arxiv.org/pdf/2305.17926))\n",
        "\n",
        "- Balanced Position Calibration (BPC): Evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs ([Source](https://arxiv.org/pdf/2305.17926)).\n",
        "\n",
        "Fore more detailed discussion on positional bias check out these two papers:\n",
        "\n",
        "- [Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs](https://arxiv.org/pdf/2406.07791v1)\n",
        "- [Large Language Models are not Fair Evaluators](https://arxiv.org/pdf/2305.17926)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX2pBHSaJ7nw"
      },
      "source": [
        "## Problem 2: Verbosity Bias\n",
        "\n",
        "LLM evaluators often exhibit verbosity bias, where they favor outputs that are more verbose, regardless of their actual quality or relevance. This bias arises because longer outputs can appear more comprehensive, detailed, or authoritative, even when they add unnecessary information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP2ht0Fd9nyL"
      },
      "outputs": [],
      "source": [
        "# Let's create an evaluator that judges correctness of a single answer\n",
        "CORRECTNESS_PROMPT = \"\"\"Given a math question and the student's answer, determine if the answer is correct.\n",
        "\n",
        "Question: {question}\n",
        "Student Answer: {answer}\n",
        "\n",
        "Is this answer correct? Respond with JUST \"YES\" or \"NO\".\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CorrectnessEvaluator(Model):\n",
        "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "    judge_prompt: str = CORRECTNESS_PROMPT\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str, correct: str) -> dict:\n",
        "        response = self.model.generate_content(\n",
        "            self.judge_prompt.format(question=question, answer=correct),\n",
        "        )\n",
        "\n",
        "        result = response.text.strip(\" \\n\")\n",
        "        return result\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def is_correct(output: str) -> bool:\n",
        "    return output == \"YES\"\n",
        "\n",
        "\n",
        "evaluation = Evaluation(dataset=mmlu_maths.rows, scorers=[is_correct])\n",
        "\n",
        "correctness_evaluator = CorrectnessEvaluator()\n",
        "plain_answer = asyncio.run(evaluation.evaluate(correctness_evaluator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ExM-0MiMVHP"
      },
      "outputs": [],
      "source": [
        "# Let's create an evaluator that judges correctness of a single answer\n",
        "CORRECTNESS_PROMPT = \"\"\"Given a math question and the student's answer, determine if the answer is correct.\n",
        "\n",
        "Question: {question}\n",
        "Student Answer: {answer}\n",
        "\n",
        "Is this answer correct? Respond with JUST \"YES\" or \"NO\".\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CorrectnessEvaluator(Model):\n",
        "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "    judge_prompt: str = CORRECTNESS_PROMPT\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str, correct: str) -> dict:\n",
        "        beautified_answer_prompt = \"\"\"You are given a math question and the correct answer to that question.\n",
        "        Can you expand on the answer by adding false reasoning steps that led to the answer?\n",
        "        Keep the correct answer at the end but add wrong/misleading calculations that led to that answer.\n",
        "        Question: {question}\n",
        "        Answer: {answer}\n",
        "        \"\"\"\n",
        "        _fake_answer = self.model.generate_content(\n",
        "            beautified_answer_prompt.format(question=question, answer=correct),\n",
        "        )\n",
        "\n",
        "        # In case the model fails to generate a fake answer, we use the correct answer as the fake answer.\n",
        "        # The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned.\n",
        "        try:\n",
        "            beautified_answer = _fake_answer.text.strip(\" \\n\")\n",
        "        except:\n",
        "            beautified_answer = f\"The correct answer is {correct}.\"\n",
        "\n",
        "        response = self.model.generate_content(\n",
        "            self.judge_prompt.format(\n",
        "                question=question,\n",
        "                answer=beautified_answer,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        result = response.text.strip(\" \\n\")\n",
        "        return result\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def is_correct(output: str) -> bool:\n",
        "    return output == \"YES\"\n",
        "\n",
        "\n",
        "evaluation = Evaluation(dataset=mmlu_maths.rows, scorers=[is_correct])\n",
        "\n",
        "correctness_evaluator = CorrectnessEvaluator()\n",
        "beautified_answer = asyncio.run(evaluation.evaluate(correctness_evaluator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuNc94aMPFG0"
      },
      "outputs": [],
      "source": [
        " print(\n",
        "    \"What's the difference in acccuracy becasue of verbosity bias?\\n\",\n",
        "    beautified_answer[\"is_correct\"][\"true_fraction\"] - plain_answer[\"is_correct\"][\"true_fraction\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oic0odpdlA5T"
      },
      "source": [
        "We can mitigate verbosity bias by explicitly instructing the LLM judge not to favor longer responses and to focus on the quality and conciseness of the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPJ8WTz1lVMO"
      },
      "source": [
        "## Problem 3: Misinformation Oversight Bias\n",
        "\n",
        "This refers to the tendency to overlook the factual errors in an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfqPsBiBZYc8"
      },
      "outputs": [],
      "source": [
        "JUDGE_PROMPT = \"\"\"You are an expert evaluator. Given a question and an answer, you need to determine if the answer is correct or incorrect.\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\n",
        "Respond with exactly one word - either \"correct\" or \"incorrect\".\"\"\"\n",
        "\n",
        "\n",
        "class MisinformationEvaluator(Model):\n",
        "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "    judge_prompt: str = JUDGE_PROMPT\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str, answer: str) -> dict:\n",
        "        response = self.model.generate_content(\n",
        "            self.judge_prompt.format(\n",
        "                question=question,\n",
        "                answer=answer,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        result = response.text.strip(\" \\n\")\n",
        "        return result\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def is_correct(output: str) -> bool:\n",
        "    return output.lower() == \"correct\"\n",
        "\n",
        "\n",
        "rag_dataset = weave.ref(\n",
        "    \"weave:///eval-course/eval-course-dev/object/rag_dataset:0ZOviYESbFjXbfXgB5LaA8zpi6GlYj391FgRL31DpWM\"\n",
        ").get()\n",
        "\n",
        "evaluation = Evaluation(dataset=rag_dataset.rows, scorers=[is_correct])\n",
        "\n",
        "misinformation_evaluator = MisinformationEvaluator()\n",
        "misinformation_results = asyncio.run(evaluation.evaluate(misinformation_evaluator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPkU-K1nlR4Q"
      },
      "outputs": [],
      "source": [
        "JUDGE_PROMPT = \"\"\"You are an expert evaluator. Given a question and an answer, you need to determine if the answer is correct or incorrect.\n",
        "You are also given the context that led to the answer.\n",
        "\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer: {answer}\n",
        "\n",
        "Respond with exactly one word - either \"correct\" or \"incorrect\".\"\"\"\n",
        "\n",
        "\n",
        "class MisinformationEvaluator(Model):\n",
        "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "    judge_prompt: str = JUDGE_PROMPT\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, context: str, question: str, answer: str) -> dict:\n",
        "        _slight_factual_error = \"\"\"You are given a response from an LLM system. You are also given the context\n",
        "        which was used to generate the answer. Can you add a slight factual error in the response. Don't change the answer too much.\n",
        "        Don't change the length of the answer. Just slightly add slight factual error. The factual error should not be noticeable easily.\n",
        "        Context: {context}\n",
        "        Answer: {answer}\n",
        "        \"\"\"\n",
        "        _factual_error_answer = self.model.generate_content(\n",
        "            _slight_factual_error.format(context=context, answer=answer),\n",
        "        )\n",
        "\n",
        "        # In case the model fails to generate a fake answer, we use the correct answer as the fake answer.\n",
        "        # The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned.\n",
        "        try:\n",
        "            _factual_error_answer = _factual_error_answer.text.strip(\" \\n\")\n",
        "        except:\n",
        "            _factual_error_answer = f\"The incorrect answer is {answer}.\"\n",
        "        response = self.model.generate_content(\n",
        "            self.judge_prompt.format(\n",
        "                question=question,\n",
        "                context=context,\n",
        "                answer=_factual_error_answer,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        result = response.text.strip(\" \\n\")\n",
        "        return result\n",
        "\n",
        "misinformation_evaluator = MisinformationEvaluator(judge_prompt=JUDGE_PROMPT)\n",
        "misinformation_results = asyncio.run(evaluation.evaluate(misinformation_evaluator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY0PIYpjoedX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
